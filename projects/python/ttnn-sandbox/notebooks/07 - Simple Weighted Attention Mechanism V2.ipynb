{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e184f9ca",
   "metadata": {},
   "source": [
    "# Simple Weighted Attention Mechanism V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf5d55",
   "metadata": {},
   "source": [
    "## Torch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7666ccfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x75bcd7f4aa10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e138cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    attn_scores = queries @ keys.T\n",
    "    attn_scores_scaled = attn_scores / (keys.shape[-1]  ** 0.5)\n",
    "    attn_weights = torch.softmax(attn_scores_scaled, dim=-1)\n",
    "\n",
    "    context_vecs = attn_weights @ values\n",
    "\n",
    "    return context_vecs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af3c990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your journey starts with one step\n",
    "context = torch.tensor(\n",
    "  [\n",
    "    [0.43, 0.15, 0.86], # Your\n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts \n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55] # step\n",
    "  ]\n",
    ")\n",
    "\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062469d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in = context[0].shape[0]\n",
    "d_out = context[0].shape[0]\n",
    "\n",
    "d_in, d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4eb131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4155, -0.4935,  0.0475],\n",
       "        [ 0.4146, -0.4936,  0.0481],\n",
       "        [ 0.4146, -0.4937,  0.0481],\n",
       "        [ 0.4165, -0.4985,  0.0489],\n",
       "        [ 0.4169, -0.4975,  0.0483],\n",
       "        [ 0.4158, -0.4975,  0.0489]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "context_vecs = sa_v2(context)\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddf7c4",
   "metadata": {},
   "source": [
    "## TTNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d67af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:01:59.362 | DEBUG    | ttnn.library_tweaks:prepare_dir_as_metal_home:54 - Existing installation of 0.57.0rc60+any detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:01:59.389 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/avgdev/.cache/ttnn,model_cache_path=/home/avgdev/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Device | INFO     | Opening user mode device driver\n",
      "\u001b[32m2025-05-18 11:01:59.436\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\n",
      "\u001b[32m2025-05-18 11:01:59.444\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-18 11:01:59.445\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Harvesting mask for chip 0 is 0x200 (physical layout: 0x1, logical: 0x200, simulated harvesting mask: 0x0).\n",
      "\u001b[32m2025-05-18 11:01:59.446\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-18 11:01:59.446\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-05-18 11:01:59.446\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[32m2025-05-18 11:01:59.455\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0)\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New chip! We now have 1 chips\n",
      "Chip initialization complete (found )\n",
      "Chip initializing complete...\n",
      " ARC\n",
      "\n",
      " [4/4] DRAM\n",
      "\n",
      " [16/16] ETH\n",
      "\n",
      " CPU\n",
      "\n",
      "Chip detection complete (found )\n"
     ]
    }
   ],
   "source": [
    "import ttnn\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ffb03",
   "metadata": {},
   "source": [
    "The goal of porting over our `torch` `SelfAttention_v2` class is to port the implementation over line-by-line to `ttnn`.\n",
    "\n",
    "Let's consider this line here:\n",
    "```python\n",
    "attn_scores = queries @ keys.T\n",
    "```\n",
    "\n",
    "Porting to `ttnn` would involve -- in the naive way:\n",
    "1. `torch` - Assign `queries` to be the result of the linear transformation with `W_query`. `queries = W_query(x)`\n",
    "2. `torch` - Transpose the `keys` (result of linear transformation with `W_key` and `x`) - we'll call this `keys_transposed`\n",
    "3. `ttnn` - Create the `ttnn` tensors from `queries` and `keys_transposed` - get back `queries_ttnn` and `keys_transposed_ttnn`\n",
    "4. `ttnn` - Compute the multiplication of the tensors using `ttnn.matmul` (`queries_ttnn` and `keys_transposed_ttnn`) -- since there is no bias, we don't have to worry about this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3aa13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[-0.22168, -0.23926,  ...,  0.11182, -0.31250],\n",
       "              [-0.31641, -0.35547,  ..., -0.02905, -0.33594],\n",
       "              ...,\n",
       "              [-0.13184, -0.13965,  ...,  0.06641, -0.18164],\n",
       "              [-0.23730, -0.27539,  ..., -0.09424, -0.21484]], shape=Shape([6, 6]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " tensor([[-0.2219, -0.2409, -0.2256, -0.1580,  0.1130, -0.3137],\n",
       "         [-0.3159, -0.3568, -0.3446, -0.2067, -0.0292, -0.3369],\n",
       "         [-0.3108, -0.3506, -0.3384, -0.2037, -0.0245, -0.3337],\n",
       "         [-0.1767, -0.2026, -0.1975, -0.1128, -0.0502, -0.1695],\n",
       "         [-0.1322, -0.1401, -0.1312, -0.0915,  0.0668, -0.1827],\n",
       "         [-0.2371, -0.2754, -0.2699, -0.1499, -0.0948, -0.2134]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch operations (steps 1 and 2)\n",
    "queries = sa_v2.W_query(context)\n",
    "keys = sa_v2.W_key(context)\n",
    "keys_transposed = keys.T\n",
    "\n",
    "# ttnn operations (steps 3)\n",
    "queries_ttnn = ttnn.from_torch(\n",
    "  queries,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "keys_transposed_ttnn = ttnn.from_torch(\n",
    "  keys_transposed,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "# ttnn.matmul to find the attention scores\n",
    "attn_scores_ttnn = ttnn.matmul(\n",
    "  queries_ttnn,\n",
    "  keys_transposed_ttnn\n",
    ")\n",
    "\n",
    "# compute the torch version too for comparison\n",
    "attn_scores_torch = queries @ keys.T\n",
    "\n",
    "attn_scores_ttnn, attn_scores_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c23c84",
   "metadata": {},
   "source": [
    "Now, the operation that we are trying to translate from `torch` to `ttnn` will be:\n",
    "\n",
    "```python\n",
    "attn_scores_scaled = attn_scores / (keys.shape[-1] ** 0.5)\n",
    "attn_weights = torch.softmax(attn_scores_scaled, dim=-1)\n",
    "context_vecs = attn_weights @ values\n",
    "```\n",
    "\n",
    "1. First get the scaled version of `attn_scores_scaled` in `ttnn`. That is using maybe the `ttnn.div` -- call the result `attn_scores_scaled_ttnn`.\n",
    "2. Then perform a `ttnn.softmax` of `attn_scores_scaled_ttnn`. -- call the result `attn_weights_ttnn`.\n",
    "3. Get `values_ttnn` by performing the transformation of `W_value` and `x` and converting that to a `ttnn.tensor`.\n",
    "4. Compute the `context_vecs_ttnn` by calling `ttnn.matmul` on `attn_weights_ttnn` and `values_ttnn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df4fee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: reader_unary_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_interleaved_start_id, eltwise_sfpu\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,5,6,7,11. First unused index: 1. Kernels: writer_unary_interleaved_start_id_blocked_sm, reader_unary_interleaved_sm, softmax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[ 0.41406, -0.49219,  0.04761],\n",
       "              [ 0.41211, -0.49219,  0.04810],\n",
       "              ...,\n",
       "              [ 0.41602, -0.49609,  0.04834],\n",
       "              [ 0.41211, -0.49414,  0.04834]], shape=Shape([6, 3]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " tensor([[ 0.4155, -0.4935,  0.0475],\n",
       "         [ 0.4146, -0.4936,  0.0481],\n",
       "         [ 0.4146, -0.4937,  0.0481],\n",
       "         [ 0.4165, -0.4985,  0.0489],\n",
       "         [ 0.4169, -0.4975,  0.0483],\n",
       "         [ 0.4158, -0.4975,  0.0489]], grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_scaled_ttnn = ttnn.div(\n",
    "  attn_scores_ttnn,\n",
    "  (keys.shape[-1] ** 0.5)\n",
    ")\n",
    "attn_weights_ttnn = ttnn.softmax(\n",
    "  attn_scores_scaled_ttnn,\n",
    "  dim=-1\n",
    ")\n",
    "\n",
    "values = sa_v2.W_value(context)\n",
    "values_ttnn = ttnn.from_torch(\n",
    "  values,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "context_vecs_ttnn = ttnn.matmul(attn_weights_ttnn, values_ttnn)\n",
    "\n",
    "# compute the torch version for comparison\n",
    "context_vecs = (\n",
    "  torch.softmax(\n",
    "    attn_scores_torch / (keys.shape[-1] ** 0.5),\n",
    "    dim=-1\n",
    "  )\n",
    ") @ sa_v2.W_value(context)\n",
    "\n",
    "context_vecs_ttnn, context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b127882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18901084",
   "metadata": {},
   "source": [
    "## SelfAttention_ttnn_v2 Implementation (naive way)\n",
    "\n",
    "This is still going to be a `torch` module. It is just that now, when we do a forward pass, we will be using our accelerator. (Wormhole n150d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692b638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_ttnn_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, x):\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    queries_ttnn = ttnn.from_torch(\n",
    "      queries,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device\n",
    "    )\n",
    "    values_ttnn = ttnn.from_torch(\n",
    "      values,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device\n",
    "    )\n",
    "    keys_ttnn = ttnn.from_torch(\n",
    "      keys,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device\n",
    "    )\n",
    "    keys_transposed_ttnn = ttnn.permute(keys_ttnn, (1, 0))\n",
    "\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "      queries_ttnn,\n",
    "      keys_transposed_ttnn\n",
    "    )\n",
    "    attn_scores_scaled_ttnn = ttnn.div(\n",
    "      attn_scores_ttnn,\n",
    "      (keys_ttnn.shape[-1] ** 0.5)\n",
    "    )\n",
    "\n",
    "    attn_weights_ttnn = ttnn.softmax(attn_scores_scaled_ttnn, dim=-1)\n",
    "\n",
    "    context_vecs_ttnn = ttnn.matmul(attn_weights_ttnn, values_ttnn)\n",
    "\n",
    "    context_vecs = ttnn.to_torch(context_vecs_ttnn)\n",
    "\n",
    "    return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16f2990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_transpose_wh_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_transpose_wh_interleaved_start_id, transpose_wh\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TorchTensor([[ 0.4141, -0.4922,  0.0476],\n",
       "             [ 0.4121, -0.4922,  0.0481],\n",
       "             [ 0.4141, -0.4922,  0.0481],\n",
       "             [ 0.4141, -0.4961,  0.0486],\n",
       "             [ 0.4160, -0.4961,  0.0483],\n",
       "             [ 0.4121, -0.4941,  0.0483]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_v2(d_in, d_out, device)\n",
    "context_vecs = sa_ttnn_v2(context)\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84dd03a",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623416d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do something\n",
      "Time: 0.1919269561767578 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(\"do something\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time: {(end - start) * 1000} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b804e3",
   "metadata": {},
   "source": [
    "## PerfTimer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "997a01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class PerfTimer:\n",
    "  def __init__(self):\n",
    "    self.start_time = 0\n",
    "    self.end_time = 0\n",
    "\n",
    "  def start(self):\n",
    "    self.start_time = time.time()\n",
    "\n",
    "  def stop(self):\n",
    "    self.end_time = time.time()\n",
    "\n",
    "  def reset(self):\n",
    "    self.start_time = 0 \n",
    "    self.end_time = 0\n",
    "\n",
    "  def elapsed_ms(self):\n",
    "    return (self.end_time - self.start_time) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e136c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-4.8013e-01, -8.4702e-01, -3.7162e-01,  ..., -4.2555e-01,\n",
       "            9.3356e-01, -3.1994e-01],\n",
       "          [ 3.3288e-01, -8.0071e-01,  1.2809e+00,  ..., -3.9552e-01,\n",
       "           -1.0113e+00,  2.8377e-02],\n",
       "          [-3.2341e-01,  7.0033e-01, -1.3483e+00,  ...,  6.5519e-01,\n",
       "           -1.1647e-03, -1.5051e-01],\n",
       "          ...,\n",
       "          [ 4.8435e-01, -1.3057e+00, -6.1837e-01,  ..., -8.0291e-01,\n",
       "            7.2481e-01, -1.0800e+00],\n",
       "          [ 1.6447e+00, -9.2544e-01,  1.9768e-01,  ...,  2.4887e-01,\n",
       "            8.5295e-01,  2.5563e+00],\n",
       "          [-1.5383e+00,  2.5991e-01,  3.0148e-01,  ...,  5.2626e-01,\n",
       "           -1.5371e-02,  4.7171e-01]],\n",
       " \n",
       "         [[-1.1993e+00,  1.2636e+00, -8.1220e-01,  ..., -1.0694e+00,\n",
       "           -8.3148e-02,  1.2506e+00],\n",
       "          [ 8.3910e-01,  5.4396e-01, -3.2130e-02,  ...,  2.6796e-02,\n",
       "            1.5036e+00, -1.6100e-01],\n",
       "          [-3.5643e-01, -1.5594e-01, -3.7622e-01,  ..., -3.5300e-01,\n",
       "            3.0946e-01,  7.7755e-01],\n",
       "          ...,\n",
       "          [ 6.6335e-03,  1.4707e+00,  3.3887e-01,  ...,  1.5722e+00,\n",
       "           -2.3351e-01,  7.3105e-01],\n",
       "          [-6.2802e-01,  2.3989e-01,  7.1889e-01,  ...,  3.0301e-01,\n",
       "            8.3691e-01, -1.1852e+00],\n",
       "          [ 2.7360e-01,  2.8683e+00,  8.6032e-01,  ..., -2.9548e-01,\n",
       "           -1.4236e+00,  9.4539e-01]],\n",
       " \n",
       "         [[ 1.4349e-01,  5.6548e-01, -6.9986e-01,  ...,  7.1795e-01,\n",
       "           -1.6810e+00, -4.3908e-01],\n",
       "          [-1.9185e+00, -8.1817e-01, -1.4340e-01,  ...,  7.1612e-01,\n",
       "            1.3230e+00, -1.3692e-04],\n",
       "          [-1.5848e-01,  6.1732e-02, -9.0593e-01,  ...,  8.7160e-01,\n",
       "            5.3884e-01,  1.1616e-01],\n",
       "          ...,\n",
       "          [-7.4033e-01, -5.8919e-01,  1.7534e-02,  ...,  1.1002e+00,\n",
       "           -1.0468e+00, -1.6949e-01],\n",
       "          [-8.6122e-01, -1.7145e+00,  2.6587e-01,  ..., -8.1361e-01,\n",
       "           -1.4157e-01,  4.5277e-01],\n",
       "          [ 1.1980e+00,  2.6229e-01,  1.2509e-01,  ...,  9.3127e-01,\n",
       "            8.7727e-01, -9.7715e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 7.5585e-02,  1.2298e+00, -1.2082e+00,  ..., -4.6441e-01,\n",
       "            2.6920e-01,  9.0262e-01],\n",
       "          [ 5.0818e-01,  2.5837e+00,  7.8114e-02,  ...,  5.9609e-01,\n",
       "            4.8680e-01,  8.3637e-01],\n",
       "          [-2.3317e+00,  6.2635e-01, -5.9066e-01,  ...,  3.4983e-01,\n",
       "            1.0885e+00,  2.0726e-01],\n",
       "          ...,\n",
       "          [ 8.9136e-01, -1.6236e+00, -1.6112e+00,  ..., -1.0600e+00,\n",
       "            6.8329e-01, -7.0106e-02],\n",
       "          [-4.6555e-01, -4.6460e-01,  4.4620e-01,  ..., -1.1367e+00,\n",
       "           -1.3831e+00, -4.0835e-01],\n",
       "          [ 1.3688e+00,  1.2069e+00,  9.2807e-01,  ..., -1.6772e+00,\n",
       "           -5.5344e-02,  8.4820e-01]],\n",
       " \n",
       "         [[ 1.0193e+00, -1.7613e+00,  1.0783e+00,  ..., -5.2507e-01,\n",
       "            1.1917e+00,  8.1378e-01],\n",
       "          [ 5.2676e-01,  1.1731e+00,  3.4437e-01,  ..., -6.3282e-01,\n",
       "            1.0222e+00,  5.2672e-01],\n",
       "          [-1.4623e-01,  1.1100e+00, -3.9509e-01,  ..., -1.7735e+00,\n",
       "            1.6343e+00,  1.2449e+00],\n",
       "          ...,\n",
       "          [-1.1463e+00, -8.4225e-01, -2.5574e-01,  ...,  7.0130e-01,\n",
       "           -8.3111e-01, -9.1557e-01],\n",
       "          [-1.6007e-01,  7.8733e-01,  1.7432e+00,  ...,  1.2305e-01,\n",
       "            8.0403e-03,  7.7788e-01],\n",
       "          [-1.3868e+00, -4.2659e-01,  1.6493e-01,  ..., -1.1971e+00,\n",
       "            2.1825e-01, -2.9969e-01]],\n",
       " \n",
       "         [[-1.4797e+00, -1.2240e-02,  1.3301e+00,  ..., -1.8861e+00,\n",
       "           -1.1249e+00, -1.1123e+00],\n",
       "          [-4.9032e-01,  7.8773e-02,  2.6251e+00,  ..., -2.7109e-01,\n",
       "            7.3186e-01, -9.6937e-01],\n",
       "          [ 2.9665e-01,  2.7594e-01, -1.4875e+00,  ..., -6.1715e-01,\n",
       "           -1.3365e+00,  9.6381e-01],\n",
       "          ...,\n",
       "          [ 8.7975e-01, -9.2430e-01, -8.8848e-01,  ...,  9.2132e-04,\n",
       "           -6.4935e-01, -1.1826e+00],\n",
       "          [ 1.1381e+00, -1.4852e+00, -3.6117e-01,  ...,  6.3340e-01,\n",
       "           -6.6709e-01, -5.0519e-01],\n",
       "          [ 2.0057e+00, -8.1796e-01, -3.8162e-01,  ...,  1.6313e+00,\n",
       "            2.5980e-02, -5.5879e-01]]]),\n",
       " torch.Size([1000, 1024, 2048]),\n",
       " 7095.6315994262695)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "timer = PerfTimer()\n",
    "\n",
    "timer.start()\n",
    "torch_tensors = torch.stack([torch.randn(1024, 2048) for _ in range(0, 1000)])\n",
    "timer.stop()\n",
    "\n",
    "torch_tensors, torch_tensors.shape, timer.elapsed_ms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afb941",
   "metadata": {},
   "source": [
    "## SelfAttention_v2 on CPU!\n",
    "\n",
    "Start power consumption: 138 W\n",
    "\n",
    "During benchmark: 223 W\n",
    "\n",
    "Delta: 85W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f43033da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0036,  0.0017, -0.0037,  ..., -0.0043,  0.0384, -0.0250],\n",
       "         [-0.0086,  0.0101, -0.0003,  ..., -0.0247,  0.0419, -0.0155],\n",
       "         [-0.0135,  0.0110,  0.0039,  ..., -0.0118,  0.0421, -0.0184],\n",
       "         ...,\n",
       "         [-0.0096,  0.0183, -0.0006,  ..., -0.0108,  0.0271, -0.0035],\n",
       "         [-0.0272,  0.0066, -0.0028,  ..., -0.0029,  0.0252, -0.0051],\n",
       "         [-0.0112,  0.0099,  0.0088,  ..., -0.0091,  0.0400, -0.0190]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " 56910.855293273926)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.reset()\n",
    "\n",
    "timer.start()\n",
    "sa_v2 = SelfAttention_v2(2048, 2048)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_v2(tensor)\n",
    "timer.stop()\n",
    "\n",
    "result, timer.elapsed_ms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451fa18",
   "metadata": {},
   "source": [
    "## SelfAttention_v2 on TTNN! (naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bc7f4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84fdb38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[ 0.0082, -0.0031, -0.0339,  ...,  0.0028, -0.0325, -0.0008],\n",
       "              [ 0.0098,  0.0149, -0.0386,  ...,  0.0262, -0.0243,  0.0126],\n",
       "              [ 0.0058,  0.0100, -0.0364,  ...,  0.0308, -0.0303,  0.0040],\n",
       "              ...,\n",
       "              [-0.0140,  0.0063, -0.0513,  ...,  0.0222, -0.0427,  0.0064],\n",
       "              [ 0.0044, -0.0039, -0.0277,  ...,  0.0074, -0.0302,  0.0047],\n",
       "              [ 0.0117,  0.0048, -0.0332,  ...,  0.0093, -0.0347,  0.0126]],\n",
       "             dtype=torch.bfloat16),\n",
       " 69427.5848865509)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.reset()\n",
    "\n",
    "timer.start()\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_v2(2048, 2048, device)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_ttnn_v2(tensor)\n",
    "timer.stop()\n",
    "\n",
    "result, timer.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d82787",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff4784",
   "metadata": {},
   "source": [
    "## Optimized SelfAttention_ttnn_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eece7d",
   "metadata": {},
   "source": [
    "### Memory Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3d6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 10:38:26.457 | DEBUG    | ttnn.tracer:visualize:443 - Dumping graph of the model to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                 Always | INFO     | Begin op: tt::tt_metal::detail::convert_python_tensor_to_tt_tensor\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<pybind11::handle const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Layout>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Tile> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | End op: tt::tt_metal::detail::convert_python_tensor_to_tt_tensor\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | Begin op: ttnn::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::MemoryConfig> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | Begin op: Tensor::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | End op: Tensor::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | End op: ttnn::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | Begin op: Tensor::to_device\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | End op: Tensor::to_device\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"229pt\" height=\"244pt\"\n",
       " viewBox=\"0.00 0.00 229.00 244.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 240)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-240 225,-240 225,4 -4,4\"/>\n",
       "<!-- torch_input_6 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>torch_input_6</title>\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"201,-236 110,-236 110,-158 201,-158 201,-236\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111.5,-209 111.5,-235 200.5,-235 200.5,-209 111.5,-209\"/>\n",
       "<text text-anchor=\"start\" x=\"128.5\" y=\"-225\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.randn</text>\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-214\" font-family=\"Linux libertine\" font-size=\"10.00\">duration: 598.4 Âµs</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111.5,-159 111.5,-208 200.5,-208 200.5,-159 111.5,-159\"/>\n",
       "<text text-anchor=\"start\" x=\"135\" y=\"-198\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"132\" y=\"-187\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 128)</text>\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-176\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "</g>\n",
       "<!-- ttnn.from_torch_11 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>ttnn.from_torch_11</title>\n",
       "<g id=\"a_node2\"><a xlink:href=\"/operation_buffer_report/2\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"221,-111 0,-111 0,0 221,0 221,-111\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1.5,-0.5 1.5,-109.5 90.5,-109.5 90.5,-0.5 1.5,-0.5\"/>\n",
       "<text text-anchor=\"start\" x=\"3.5\" y=\"-52.5\" font-family=\"Linux libertine\" font-size=\"10.00\">2: ttnn.from_torch</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"91.5,-60.5 91.5,-109.5 220.5,-109.5 220.5,-60.5 91.5,-60.5\"/>\n",
       "<text text-anchor=\"start\" x=\"139\" y=\"-99.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"132\" y=\"-88.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 128)</text>\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-77.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"91.5,-0.5 91.5,-59.5 220.5,-59.5 220.5,-0.5 91.5,-0.5\"/>\n",
       "<text text-anchor=\"start\" x=\"135\" y=\"-49.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-38.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([256, 128])</text>\n",
       "<text text-anchor=\"start\" x=\"108\" y=\"-27.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-16.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-5.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: L1, INTERLEAVED</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- torch_input_6&#45;&gt;ttnn.from_torch_11 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>torch_input_6:#0&#45;&gt;ttnn.from_torch_11:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.5,-158C156.5,-140.68 156.5,-134.02 156.5,-120.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160,-120.5 156.5,-110.5 153,-120.5 160,-120.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-132\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x798bec1d2a40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TTNN_CONFIG_OVERRIDES\"] = \"{\\\"enable_fast_runtime_mode\\\": false}\" \n",
    "\n",
    "import ttnn\n",
    "import torch\n",
    "from ttnn.tracer import trace, visualize\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "with trace():\n",
    "  test_ttnn = ttnn.from_torch(\n",
    "    torch.randn(256, 128),\n",
    "    dtype=ttnn.bfloat16,\n",
    "    layout=ttnn.TILE_LAYOUT,\n",
    "    device=device,\n",
    "    memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "  )\n",
    "\n",
    "visualize(test_ttnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6119f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ttnn.close_device(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7b230",
   "metadata": {},
   "source": [
    "Start power consumption: 139 W\n",
    "\n",
    "During power consumption: 215 W\n",
    "\n",
    "Delta = 76 W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67bf88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1f5322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_ttnn_optimized_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "    self.W_query_ttnn = ttnn.from_torch(\n",
    "      self.W_query.weight,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "    self.W_key_ttnn = ttnn.from_torch(\n",
    "      self.W_key.weight,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "    self.W_value_ttnn = ttnn.from_torch(\n",
    "      self.W_value.weight,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "\n",
    "    self.scalar = 1 / (d_out ** 0.5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_ttnn = ttnn.from_torch(\n",
    "      x,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "    )\n",
    "\n",
    "    queries_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_query_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    )\n",
    "    values_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_value_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    )\n",
    "    keys_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_key_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    )\n",
    "\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "      queries_ttnn,\n",
    "      ttnn.permute(keys_ttnn, (1, 0)),\n",
    "      core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    )\n",
    "    attn_scores_scaled_ttnn = attn_scores_ttnn * self.scalar\n",
    "    attn_weights_ttnn = ttnn.softmax(attn_scores_scaled_ttnn, dim=-1)\n",
    "\n",
    "    context_vecs_ttnn = ttnn.matmul(\n",
    "      attn_weights_ttnn,\n",
    "      values_ttnn,\n",
    "      core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    )\n",
    "\n",
    "    context_vecs = ttnn.to_torch(context_vecs_ttnn)\n",
    "\n",
    "    return context_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4259717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Enabling program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "ttnn.enable_program_cache(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8a4993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[ 1.4648e-02, -1.2085e-02, -1.7334e-02,  ...,  3.0640e-02,\n",
       "               -2.6855e-02,  3.4180e-02],\n",
       "              [ 1.4038e-02, -3.4027e-03, -2.9053e-02,  ...,  2.6001e-02,\n",
       "               -2.8442e-02,  2.3560e-02],\n",
       "              [ 4.0588e-03, -1.1108e-02, -3.2471e-02,  ...,  2.5146e-02,\n",
       "               -2.5635e-02,  2.3804e-02],\n",
       "              ...,\n",
       "              [ 1.9775e-02,  1.9073e-06, -1.9653e-02,  ...,  2.4170e-02,\n",
       "               -3.3447e-02,  1.7944e-02],\n",
       "              [ 1.5564e-02,  4.6692e-03, -2.3071e-02,  ...,  2.1484e-02,\n",
       "               -3.3936e-02,  5.1270e-03],\n",
       "              [ 1.2451e-02, -7.4768e-03, -3.1006e-02,  ...,  6.6528e-03,\n",
       "               -3.0518e-02,  1.1902e-02]], dtype=torch.bfloat16),\n",
       " 8518.006086349487)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "timer.reset()\n",
    "\n",
    "timer.start()\n",
    "sa_ttnn_opt_v2 = SelfAttention_ttnn_optimized_v2(2048, 2048, device)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_ttnn_opt_v2(tensor)\n",
    "timer.stop()\n",
    "\n",
    "result, timer.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33974435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943aa4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
